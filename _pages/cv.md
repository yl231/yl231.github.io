---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

# Research Interest

**LLM, Data Management, and Data-Centric Machine Learning**

---

# Education

**Rice University, TX, USA**  
*B.S. in Computer Science, B.A. in Mathematics*  
GPA: 3.81 / 4.0 (Ranked 2nd in CS class)  
_Aug 2021 – Dec 2024_

---

# Publications

### **MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations**  
*arXiv Preprint, Aug 2024*  
**Yifan Lu**\*, Shaochen Zhong\*, Lize Shao, Bhargav Bhushanam, Xiaocong Du, Yixin Wan, Yucheng Shi, Daochen Zha, Yiwei Wang, Ninghao Liu, Kaixiong Zhou, Shuai Xu, Kai-Wei Chang, Louis Feng, Vipin Chaudhary, Xia Hu

---

### **Monarch: Scalable Butterfly Computation with One-Hop Neighborhoods**  
*In Submission, Nov 2024*  
**Yifan Lu**, Yuxin Tang, Bowen Yao, Mangesh Bendre, Mahasweta Das

---

### **An Adapter Management System For Diffusion Models**  
*2024 MLSys YPS, April 2024*  
Yuxin Tang, **Yifan Lu**, Yicheng Jiang

---

# Research Experience

### **Undergraduate Researcher**  
*LLM Multi-Hop Editing Audit and Benchmark*  
Rice University, Jan 2024 – Aug 2024  
**Advisor:** Xia Hu, Shaochen Zhong  

- Identified dataset contamination in **MQuAKE**, revealing 33–76% of cases were corrupted due to fact leakage.
- Conducted comprehensive re-benchmarking of multi-hop knowledge editing methods and evaluated their faithfulness.
- Proposed **GWalk**, a method utilizing lightweight editing knowledge graphs, outperforming SOTA methods.
- Submitted to **ICLR 2025**, peer-reviewed with a score of **6666**.

---

### **Undergraduate Researcher**  
*Distributed Large-scale Graph Computation*  
Rice University, Aug 2024 – Nov 2024  
**Advisor:** Yuxin Tang  

- Developed **Monarch**, a framework for parallel butterfly motif computation at billion-scale.
- Designed an algorithm leveraging **first-hop neighbor information**, reducing communication overhead.
- Achieved up to **5.7x faster computation** and **1.7x lower cache miss ratios** compared to existing methods.
- Submitted to **SIGMOD 2025**.

---

### **Independent Researcher**  
*Head-dimension KV Cache Compression*  
Nov 2024 – Present  

- Devised a **KV cache compression** technique by leveraging retrieval capacity of attention heads.
- Developed an **importance scoring mechanism** for adaptive allocation of KV cache budgets.
- Demonstrated promising results on **Needle-In-The-Hay** tasks and extended to **MQA models**.
- Preparing findings for **ICML 2025**.

---

### **Research Intern**  
*Lightweight Embedding for Graph Recommender*  
Rice University, Aug 2024 – Present  
**Advisor:** Huiyuan Chen  

- Designed a parameter-efficient embedding framework for graph recommender systems.
- Developed **hashing-based meta-embedding learning** with **self-attention** for distant node communication.
- Implemented a novel **two-stage training strategy** to optimize meta-embedding and attention modules.
- Achieved **4.33% and 6.25%** improvement on **Yelp 2020** and **Gowalla** datasets.
- Preparing for submission to **SIGIR 2025**.

---

# Teaching Experience

- **Algorithm and Data Structure, Rice University**  
  *Teaching Assistant, Spring 2023 – Fall 2024*  
  Instructor: Prof. Tasos Kyrillidis

- **Linear Algebra, Rice University**  
  *Teaching Assistant, Spring 2022*  
  Instructor: Prof. Stephen Wang

---

# Awards and Honors

- **Louis J. Walsh Merit Scholarship**  
  *Top 1%, $1,500*  
  May 2024

- **President's Honor Roll**  
  *Top 10%*  
  2021, 2023, 2024

---

# Technical Skills

- **Languages:** Python, Java, C/C++  
- **Tools/Packages:** ChatGPT, PyTorch, Transformers, Numpy, Pandas, LaTeX, Git
